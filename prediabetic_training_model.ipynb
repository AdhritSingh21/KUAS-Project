{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Input, BatchNormalization\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from statistics import pstdev, mean\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CGM</th>\n",
       "      <th>CGM_predict</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-02-28 12:00:00</th>\n",
       "      <td>107.000000</td>\n",
       "      <td>106.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-28 12:30:00</th>\n",
       "      <td>106.333333</td>\n",
       "      <td>108.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-28 13:00:00</th>\n",
       "      <td>108.833333</td>\n",
       "      <td>118.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-28 13:30:00</th>\n",
       "      <td>118.500000</td>\n",
       "      <td>118.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-28 14:00:00</th>\n",
       "      <td>118.666667</td>\n",
       "      <td>114.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            CGM  CGM_predict\n",
       "Time                                        \n",
       "2020-02-28 12:00:00  107.000000   106.333333\n",
       "2020-02-28 12:30:00  106.333333   108.833333\n",
       "2020-02-28 13:00:00  108.833333   118.500000\n",
       "2020-02-28 13:30:00  118.500000   118.666667\n",
       "2020-02-28 14:00:00  118.666667   114.833333"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data/visualize\n",
    "file_path = r'C:\\Users\\19176\\Desktop\\Ohio Data Set\\data\\prediabetic_file\\prediabetic_cleaned_training\\cleaned_Dexcom_006.csv'\n",
    "data = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "# Display the DataFrame\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(474, 2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_lstm_cgm(X_train, y_train):\n",
    "    n_folds = 5\n",
    "    cross_validation = KFold(n_folds)\n",
    "\n",
    "    X_data = X_train\n",
    "    y_data = y_train\n",
    "    # input_dim = X_data.shape[1]\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    scaler_x.fit(X_data)\n",
    "    scaler_y.fit(y_data)\n",
    "    scaled_X_train_data = scaler_x.transform(X_data)\n",
    "    scaled_y_train_data = scaler_y.transform(y_data)\n",
    "    scaled_X_train_data = np.reshape(scaled_X_train_data, (scaled_X_train_data.shape[0], 1, scaled_X_train_data.shape[1]))\n",
    "\n",
    "    lstm_best_score = []\n",
    "    model_check_point_callback = keras.callbacks.ModelCheckpoint(\n",
    "        filepath = 'prediabetic_lstm_cgm.h5',\n",
    "        save_best_only = True,\n",
    "        monitor = 'val_loss')\n",
    "    early_stopping = keras.callbacks.EarlyStopping(patience=100)\n",
    "\n",
    "    for train_id_x, val_id_x in cross_validation.split(scaled_X_train_data, scaled_y_train_data):\n",
    "        X_train_fold, X_val_fold = scaled_X_train_data[train_id_x], scaled_X_train_data[val_id_x]\n",
    "        y_train_fold, y_val_fold = scaled_y_train_data[train_id_x], scaled_y_train_data[val_id_x]\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(128, input_shape = (scaled_X_train_data.shape[1], scaled_X_train_data.shape[2])))\n",
    "        model.add(Dense(150, activation = 'relu'))\n",
    "        model.add(Dropout(0.20))\n",
    "        model.add(Dense(100, activation = 'relu'))\n",
    "        model.add(Dropout(0.15))\n",
    "        model.add(Dense(50, activation = 'relu'))\n",
    "        model.add(Dense(20, activation = 'relu'))\n",
    "        model.add(Dense(1, activation = 'relu'))\n",
    "        model.compile(loss = 'mse', optimizer = 'adam')\n",
    "        model.summary()\n",
    "        model.fit(X_train_fold, y_train_fold,\n",
    "                  epochs = 200, batch_size = 32, shuffle = False,\n",
    "                  verbose=1,\n",
    "                  validation_data = (X_val_fold, y_val_fold),\n",
    "                  callbacks = [early_stopping, model_check_point_callback])\n",
    "        lstm_best_score.append(model_check_point_callback.best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            CGM\n",
      "Time                           \n",
      "2020-02-28 12:00:00  107.000000\n",
      "2020-02-28 12:30:00  106.333333\n",
      "2020-02-28 13:00:00  108.833333\n",
      "2020-02-28 13:30:00  118.500000\n",
      "2020-02-28 14:00:00  118.666667\n",
      "...                         ...\n",
      "2020-03-09 06:30:00  107.500000\n",
      "2020-03-09 07:00:00  104.833333\n",
      "2020-03-09 07:30:00  103.000000\n",
      "2020-03-09 08:00:00  106.333333\n",
      "2020-03-09 08:30:00  149.833333\n",
      "\n",
      "[474 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.DataFrame(index = data.index, data = data.CGM, columns = ['CGM'])\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     CGM_predict\n",
      "Time                            \n",
      "2020-02-28 12:00:00   106.333333\n",
      "2020-02-28 12:30:00   108.833333\n",
      "2020-02-28 13:00:00   118.500000\n",
      "2020-02-28 13:30:00   118.666667\n",
      "2020-02-28 14:00:00   114.833333\n",
      "...                          ...\n",
      "2020-03-09 06:30:00   104.833333\n",
      "2020-03-09 07:00:00   103.000000\n",
      "2020-03-09 07:30:00   106.333333\n",
      "2020-03-09 08:00:00   149.833333\n",
      "2020-03-09 08:30:00   138.333333\n",
      "\n",
      "[474 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "y_train = pd.DataFrame(index = data.index, data = data.CGM_predict, columns = ['CGM_predict'])\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_10 (LSTM)              (None, 128)               66560     \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 150)               19350     \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 150)               0         \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 100)               15100     \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 20)                1020      \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107,101\n",
      "Trainable params: 107,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "12/12 [==============================] - 4s 67ms/step - loss: 0.1431 - val_loss: 0.1138\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1375 - val_loss: 0.0906\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0780 - val_loss: 0.0196\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0355 - val_loss: 0.0178\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0280 - val_loss: 0.0134\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0199 - val_loss: 0.0101\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0161 - val_loss: 0.0097\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.0098\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0152 - val_loss: 0.0094\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0147 - val_loss: 0.0094\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.0094\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.0095\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0097\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0094\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0094\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.0094\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0148 - val_loss: 0.0094\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0149 - val_loss: 0.0094\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.0094\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.0094\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0145 - val_loss: 0.0095\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0094\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.0095\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.0096\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.0094\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0094\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.0094\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.0094\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0138 - val_loss: 0.0094\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.0139 - val_loss: 0.0095\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.0096\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0095\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0097\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0151 - val_loss: 0.0095\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0151 - val_loss: 0.0093\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0154 - val_loss: 0.0094\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0094\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0094\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0094\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0094\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.0093\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0139 - val_loss: 0.0093\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0150 - val_loss: 0.0093\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0093\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0094\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0094\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0093\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0093\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0093\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0141 - val_loss: 0.0093\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.0095\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0095\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0096\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0094\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.0093\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0138 - val_loss: 0.0093\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.0139 - val_loss: 0.0093\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.0093\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.0093\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0095\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0093\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0093\n",
      "Epoch 63/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0095\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0096\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0093\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0093\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0093\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.0138 - val_loss: 0.0093\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0093\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0093\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0139 - val_loss: 0.0092\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0093\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0093\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0095\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.0093\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0093\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0093\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0094\n",
      "Epoch 79/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.0095\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0093\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.0094\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0093\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0135 - val_loss: 0.0092\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0093\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0093\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0093\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0093\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0093\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.0093\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0093\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.0093\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0092\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0093\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0093\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0094\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0093\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0094\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0094\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0096\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0094\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0093\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0093\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0093\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0094\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0093\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.0094\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0092\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0097\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0138 - val_loss: 0.0096\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0094\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0095\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0094\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0094\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0095\n",
      "Epoch 115/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0138 - val_loss: 0.0092\n",
      "Epoch 116/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0093\n",
      "Epoch 117/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0092\n",
      "Epoch 118/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0093\n",
      "Epoch 119/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0094\n",
      "Epoch 120/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.0093\n",
      "Epoch 121/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0092\n",
      "Epoch 122/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0139 - val_loss: 0.0092\n",
      "Epoch 123/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0093\n",
      "Epoch 124/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0095\n",
      "Epoch 125/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0095\n",
      "Epoch 126/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.0094\n",
      "Epoch 127/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0136 - val_loss: 0.0094\n",
      "Epoch 128/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0136 - val_loss: 0.0093\n",
      "Epoch 129/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0093\n",
      "Epoch 130/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0094\n",
      "Epoch 131/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0094\n",
      "Epoch 132/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0093\n",
      "Epoch 133/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0094\n",
      "Epoch 134/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0093\n",
      "Epoch 135/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0093\n",
      "Epoch 136/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0136 - val_loss: 0.0093\n",
      "Epoch 137/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0094\n",
      "Epoch 138/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.0094\n",
      "Epoch 139/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0094\n",
      "Epoch 140/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0094\n",
      "Epoch 141/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0093\n",
      "Epoch 142/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0093\n",
      "Epoch 143/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.0093\n",
      "Epoch 144/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.0092\n",
      "Epoch 145/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0138 - val_loss: 0.0092\n",
      "Epoch 146/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0093\n",
      "Epoch 147/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0092\n",
      "Epoch 148/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0094\n",
      "Epoch 149/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0093\n",
      "Epoch 150/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.0093\n",
      "Epoch 151/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.0094\n",
      "Epoch 152/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0092\n",
      "Epoch 153/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0093\n",
      "Epoch 154/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0093\n",
      "Epoch 155/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0094\n",
      "Epoch 156/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0093\n",
      "Epoch 157/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.0095\n",
      "Epoch 158/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0094\n",
      "Epoch 159/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0094\n",
      "Epoch 160/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0094\n",
      "Epoch 161/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.0095\n",
      "Epoch 162/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0093\n",
      "Epoch 163/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0094\n",
      "Epoch 164/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0093\n",
      "Epoch 165/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0093\n",
      "Epoch 166/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.0094\n",
      "Epoch 167/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0096\n",
      "Epoch 168/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0094\n",
      "Epoch 169/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0093\n",
      "Epoch 170/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0093\n",
      "Epoch 171/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0093\n",
      "Epoch 172/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0093\n",
      "Epoch 173/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0131 - val_loss: 0.0093\n",
      "Epoch 174/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0093\n",
      "Epoch 175/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0094\n",
      "Epoch 176/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0093\n",
      "Epoch 177/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0094\n",
      "Epoch 178/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0093\n",
      "Epoch 179/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0093\n",
      "Epoch 180/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0093\n",
      "Epoch 181/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0093\n",
      "Epoch 182/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0094\n",
      "Epoch 183/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0094\n",
      "Epoch 184/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0093\n",
      "Epoch 185/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.0093\n",
      "Epoch 186/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0094\n",
      "Epoch 187/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0094\n",
      "Epoch 188/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0094\n",
      "Epoch 189/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0093\n",
      "Epoch 190/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0093\n",
      "Epoch 191/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0094\n",
      "Epoch 192/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0094\n",
      "Epoch 193/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0093\n",
      "Epoch 194/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0094\n",
      "Epoch 195/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0094\n",
      "Epoch 196/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.0094\n",
      "Epoch 197/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.0095\n",
      "Epoch 198/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.0094\n",
      "Epoch 199/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0141 - val_loss: 0.0095\n",
      "Epoch 200/200\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.0138 - val_loss: 0.0094\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_11 (LSTM)              (None, 128)               66560     \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 150)               19350     \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 150)               0         \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 100)               15100     \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 20)                1020      \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107,101\n",
      "Trainable params: 107,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "12/12 [==============================] - 3s 59ms/step - loss: 0.1406 - val_loss: 0.1233\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0957 - val_loss: 0.0348\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0341 - val_loss: 0.0286\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0269 - val_loss: 0.0193\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0176 - val_loss: 0.0121\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0153 - val_loss: 0.0115\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.0142 - val_loss: 0.0112\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0112\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0112\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.0111\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0112\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0112\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0112\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0112\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0112\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.0112\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0112\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.0111\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.0114\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0112\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0113\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0112\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0112\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0113\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0112\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0112\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0111\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0113\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0111\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.0111\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.0111\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0113\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0111\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.0115\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0111\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0111\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0111\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0111\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0112\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0110\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.0111\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0114\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0110\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0111\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0113\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0111\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0112\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0110\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0115\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0112\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0130 - val_loss: 0.0111\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0113\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0112\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0111\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0111\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0112\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0111\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0111\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0138 - val_loss: 0.0113\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0111\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0111\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0110\n",
      "Epoch 63/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0112\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0110\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0112\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0112\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0111\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0111\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0111\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0112\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0136 - val_loss: 0.0111\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0110\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0116\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0110\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0112\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0111\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0111\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0110\n",
      "Epoch 79/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0112\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0111\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0113\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0111\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0111\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0110\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0116\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0110\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0111\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0111\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0111\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0110\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0110\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0114\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0114\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0111\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0111\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0111\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0112\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0113\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0111\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0110\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0113\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0111\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0111\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0111\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0113\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0112\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0112\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0115\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0111\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0111\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0110\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0111\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0112\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0114\n",
      "Epoch 115/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0110\n",
      "Epoch 116/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0111\n",
      "Epoch 117/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0111\n",
      "Epoch 118/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0115\n",
      "Epoch 119/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0111\n",
      "Epoch 120/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0111\n",
      "Epoch 121/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0110\n",
      "Epoch 122/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0112\n",
      "Epoch 123/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0111\n",
      "Epoch 124/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0112\n",
      "Epoch 125/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0110\n",
      "Epoch 126/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0111\n",
      "Epoch 127/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0113\n",
      "Epoch 128/200\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.0129 - val_loss: 0.0110\n",
      "Epoch 129/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0131 - val_loss: 0.0115\n",
      "Epoch 130/200\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.0131 - val_loss: 0.0110\n",
      "Epoch 131/200\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.0137 - val_loss: 0.0110\n",
      "Epoch 132/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.0110\n",
      "Epoch 133/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0111\n",
      "Epoch 134/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0130 - val_loss: 0.0111\n",
      "Epoch 135/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0112\n",
      "Epoch 136/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0111\n",
      "Epoch 137/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0111\n",
      "Epoch 138/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0111\n",
      "Epoch 139/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0111\n",
      "Epoch 140/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0112\n",
      "Epoch 141/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0111\n",
      "Epoch 142/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0110\n",
      "Epoch 143/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0110\n",
      "Epoch 144/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0110\n",
      "Epoch 145/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0110\n",
      "Epoch 146/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0111\n",
      "Epoch 147/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0111\n",
      "Epoch 148/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0110\n",
      "Epoch 149/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0110\n",
      "Epoch 150/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0112\n",
      "Epoch 151/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0111\n",
      "Epoch 152/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0114\n",
      "Epoch 153/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0111\n",
      "Epoch 154/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0111\n",
      "Epoch 155/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0112\n",
      "Epoch 156/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0132 - val_loss: 0.0115\n",
      "Epoch 157/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0110\n",
      "Epoch 158/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0112\n",
      "Epoch 159/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0110\n",
      "Epoch 160/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0113\n",
      "Epoch 161/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0110\n",
      "Epoch 162/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0110\n",
      "Epoch 163/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0111\n",
      "Epoch 164/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0110\n",
      "Epoch 165/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0131 - val_loss: 0.0111\n",
      "Epoch 166/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0110\n",
      "Epoch 167/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0112\n",
      "Epoch 168/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0110\n",
      "Epoch 169/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0112\n",
      "Epoch 170/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0111\n",
      "Epoch 171/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0111\n",
      "Epoch 172/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0111\n",
      "Epoch 173/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0112\n",
      "Epoch 174/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0111\n",
      "Epoch 175/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0110\n",
      "Epoch 176/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0112\n",
      "Epoch 177/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0111\n",
      "Epoch 178/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0111\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 128)               66560     \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 150)               19350     \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 150)               0         \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 100)               15100     \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 20)                1020      \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107,101\n",
      "Trainable params: 107,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "12/12 [==============================] - 3s 63ms/step - loss: 0.0928 - val_loss: 0.0480\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0331 - val_loss: 0.0333\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0287 - val_loss: 0.0274\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0204 - val_loss: 0.0186\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0149\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0152\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0146\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0145\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0146\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0146\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0148\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0145\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.0146\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.0145\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0145\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0136 - val_loss: 0.0146\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0148\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.0146\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0149\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0149\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0147\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0146\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0147\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0147\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0148\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0130 - val_loss: 0.0147\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0148\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0148\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0148\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0148\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0147\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0148\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0149\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0149\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0148\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0149\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0151\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0151\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0151\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0130 - val_loss: 0.0149\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0130 - val_loss: 0.0148\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0128 - val_loss: 0.0147\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0147\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0150\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0149\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0147\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0150\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0152\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0130 - val_loss: 0.0149\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0150\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0150\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.0146\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0147\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0149\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0149\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0149\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0149\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0150\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0150\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0148\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0149\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0149\n",
      "Epoch 63/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0148\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0147\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0147\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0148\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0149\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0149\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.0148\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0148\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0148\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0148\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0150\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0148\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0147\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0147\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0147\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0147\n",
      "Epoch 79/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0147\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0148\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0148\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0148\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0148\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0150\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0148\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0148\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0146\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0148\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0148\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0147\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0147\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0148\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0148\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.0148\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0148\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0149\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0147\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0147\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0148\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0148\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.0149\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0148\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0148\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0148\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0148\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0148\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0148\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0147\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0148\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0148\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.0149\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0147\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.0149\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0148\n",
      "Epoch 115/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0148\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_13 (LSTM)              (None, 128)               66560     \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 150)               19350     \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 150)               0         \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 100)               15100     \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 20)                1020      \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107,101\n",
      "Trainable params: 107,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "12/12 [==============================] - 3s 59ms/step - loss: 0.0668 - val_loss: 0.0420\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0326 - val_loss: 0.0371\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0216 - val_loss: 0.0252\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0171 - val_loss: 0.0173\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0148\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0138 - val_loss: 0.0147\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0152\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0151\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0148\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0150\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0149\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0154\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0150\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0148\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0150\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0151\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0150\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0150\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0149\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0149\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0149\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0148\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0148\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0148\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0147\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0149\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0149\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0148\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0150\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.0131 - val_loss: 0.0148\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0147\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0148\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0148\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0148\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0148\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0149\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0146\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.0146\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0146\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0146\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0145\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0147\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0148\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0146\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0146\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0149\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0149\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0147\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0147\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0148\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0148\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.0147\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0146\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0146\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0146\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0147\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0148\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0148\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0149\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0148\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0153\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0130 - val_loss: 0.0149\n",
      "Epoch 63/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0147\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0147\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0147\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0148\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0149\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0148\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0149\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0152\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0150\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0147\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0147\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0146\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0125 - val_loss: 0.0147\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.0128 - val_loss: 0.0149\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.0127 - val_loss: 0.0148\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0149\n",
      "Epoch 79/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.0150\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0146\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0148\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0147\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0148\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.0149\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0147\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0147\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0149\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0146\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0148\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0148\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0148\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0147\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0147\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0147\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0147\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0148\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0149\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0149\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0147\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0148\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0146\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0149\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0150\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0146\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0147\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0146\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0152\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0148\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0148\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0148\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0147\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0119 - val_loss: 0.0148\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0148\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0147\n",
      "Epoch 115/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0150\n",
      "Epoch 116/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0152\n",
      "Epoch 117/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0149\n",
      "Epoch 118/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0150\n",
      "Epoch 119/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0149\n",
      "Epoch 120/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0148\n",
      "Epoch 121/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0150\n",
      "Epoch 122/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0147\n",
      "Epoch 123/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0146\n",
      "Epoch 124/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0147\n",
      "Epoch 125/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0149\n",
      "Epoch 126/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0149\n",
      "Epoch 127/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0149\n",
      "Epoch 128/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0150\n",
      "Epoch 129/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.0148\n",
      "Epoch 130/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0151\n",
      "Epoch 131/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0148\n",
      "Epoch 132/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0152\n",
      "Epoch 133/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0149\n",
      "Epoch 134/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0150\n",
      "Epoch 135/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0149\n",
      "Epoch 136/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0148\n",
      "Epoch 137/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0150\n",
      "Epoch 138/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0150\n",
      "Epoch 139/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0147\n",
      "Epoch 140/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0148\n",
      "Epoch 141/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0148\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_14 (LSTM)              (None, 128)               66560     \n",
      "                                                                 \n",
      " dense_70 (Dense)            (None, 150)               19350     \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 150)               0         \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 100)               15100     \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 20)                1020      \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107,101\n",
      "Trainable params: 107,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "12/12 [==============================] - 3s 55ms/step - loss: 0.0954 - val_loss: 0.0490\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0336 - val_loss: 0.0279\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0277 - val_loss: 0.0243\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0191 - val_loss: 0.0158\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.0136\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0132\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0129\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0129\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0129\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0129\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0130\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0129\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0129\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0131\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0130\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0129\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0131\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0128\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0128\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0130\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0129\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0128\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0128\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0131\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0129\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0130\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0132\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0130\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0129\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0128\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0128\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0130\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0128\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0132\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0131\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0128\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0129\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0130\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0130\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0134\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0129\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0128\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0129\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0131\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0131 - val_loss: 0.0130\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0128\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0129\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0130\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0129\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0130\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0133\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0132\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0128\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0128\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0131\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0130\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0132\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0129\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0129\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0131\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0130\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0128\n",
      "Epoch 63/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0130\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.0137 - val_loss: 0.0137\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.0132 - val_loss: 0.0129\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0132 - val_loss: 0.0129\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0130\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0128\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0128\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0128\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0128\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0130\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0130\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0131\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0134 - val_loss: 0.0132\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0131\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0128\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0131\n",
      "Epoch 79/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0130\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0129\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0131\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0134\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0129\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0131 - val_loss: 0.0129\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0131\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0129\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0129\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0129\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0128\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0130\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0130\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0129\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0133\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0133\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0128\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0128\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0129\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0128\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0132\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0129\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0129\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0130\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0132\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0133\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0130\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0129\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0129\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0129\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0131\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0134\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0129\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0129\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0130\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0130\n",
      "Epoch 115/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0129\n",
      "Epoch 116/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0129\n",
      "Epoch 117/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0130\n",
      "Epoch 118/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0130\n",
      "Epoch 119/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0129\n",
      "Epoch 120/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0130\n",
      "Epoch 121/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0129\n",
      "Epoch 122/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0132\n",
      "Epoch 123/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0129\n",
      "Epoch 124/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0132\n",
      "Epoch 125/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0129\n",
      "Epoch 126/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0129\n",
      "Epoch 127/200\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.0131 - val_loss: 0.0129\n",
      "Epoch 128/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0128 - val_loss: 0.0130\n",
      "Epoch 129/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0133\n",
      "Epoch 130/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0129\n",
      "Epoch 131/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0130\n",
      "Epoch 132/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0130\n",
      "Epoch 133/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0129\n",
      "Epoch 134/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0129\n",
      "Epoch 135/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0129\n",
      "Epoch 136/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0129\n",
      "Epoch 137/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0129\n",
      "Epoch 138/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0129\n",
      "Epoch 139/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0130\n",
      "Epoch 140/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0129\n",
      "Epoch 141/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0129\n",
      "Epoch 142/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0130\n",
      "Epoch 143/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0132\n",
      "Epoch 144/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0130\n",
      "Epoch 145/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0131\n",
      "Epoch 146/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0131\n",
      "Epoch 147/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0131\n",
      "Epoch 148/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0129\n",
      "Epoch 149/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0129\n",
      "Epoch 150/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0129\n",
      "Epoch 151/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0129\n",
      "Epoch 152/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0130\n",
      "Epoch 153/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0132\n",
      "Epoch 154/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0132\n",
      "Epoch 155/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0129\n",
      "Epoch 156/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0130\n",
      "Epoch 157/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0130\n",
      "Epoch 158/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0129\n",
      "Epoch 159/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0129\n",
      "Epoch 160/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0131\n",
      "Epoch 161/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0129\n",
      "Epoch 162/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0132\n",
      "Epoch 163/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0131\n",
      "Epoch 164/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0130\n",
      "Epoch 165/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0130\n",
      "Epoch 166/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0130\n",
      "Epoch 167/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0130\n",
      "Epoch 168/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0130\n",
      "Epoch 169/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0129\n"
     ]
    }
   ],
   "source": [
    "train_model_lstm_cgm(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_root_mean_squared_error(true, pred):\n",
    "    squared_error = np.square((true - pred))\n",
    "    sum_squared_error = np.sum(squared_error)\n",
    "    rmse = np.sqrt(sum_squared_error / true.size)\n",
    "    nrmse_loss = round(rmse/np.std(true),3) # pred or true\n",
    "    return nrmse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_by_model(model, data, print_individual_metrics):\n",
    "    test_time = data.index\n",
    "    test_gl_value = data['CGM']\n",
    "    X_data = data.drop(columns = ['CGM_predict'])\n",
    "    y_data = data[['CGM_predict']]\n",
    "    input_dim = X_data.shape[1]\n",
    "\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    scaler_x.fit(X_data)\n",
    "    scaler_y.fit(y_data)\n",
    "\n",
    "    X_test_data = data.drop(columns = ['CGM_predict'])\n",
    "    y_test_data = data[['CGM_predict']]\n",
    "    scaled_X_test_data = scaler_x.transform(X_test_data)\n",
    "    scaled_X_test_data = np.reshape(scaled_X_test_data, (scaled_X_test_data.shape[0], 1, scaled_X_test_data.shape[1]))\n",
    "    prediction = model.predict(scaled_X_test_data, batch_size = 32)\n",
    "    scaled_prediction = scaler_y.inverse_transform(prediction)\n",
    "\n",
    "    mae = mean_absolute_error(scaled_prediction, y_test_data)\n",
    "    rmse = math.sqrt(mean_squared_error(scaled_prediction, y_test_data))\n",
    "    nrmse = normalized_root_mean_squared_error(scaled_prediction, y_test_data.values)\n",
    "\n",
    "    if print_individual_metrics == True:\n",
    "        print(f\"MAE: {round(mae,3)}\")\n",
    "        print(f\"RMSE: {round(rmse,3)}\")\n",
    "        print(f\"NRMSE: {round(nrmse,3)}\")\n",
    "\n",
    "    return (mae,\n",
    "            rmse,\n",
    "            nrmse,\n",
    "            y_test_data.values,\n",
    "            scaled_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 1ms/step\n",
      "MAE: 9.722\n",
      "RMSE: 15.107\n",
      "NRMSE: 0.65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9.721736789923344,\n",
       " 15.106688152191985,\n",
       " 0.65,\n",
       " array([[106.33333333],\n",
       "        [108.83333333],\n",
       "        [118.5       ],\n",
       "        [118.66666667],\n",
       "        [114.83333333],\n",
       "        [107.        ],\n",
       "        [103.33333333],\n",
       "        [100.16666667],\n",
       "        [109.5       ],\n",
       "        [122.16666667],\n",
       "        [114.33333333],\n",
       "        [105.66666667],\n",
       "        [109.5       ],\n",
       "        [109.16666667],\n",
       "        [111.66666667],\n",
       "        [136.33333333],\n",
       "        [132.5       ],\n",
       "        [122.66666667],\n",
       "        [131.83333333],\n",
       "        [126.83333333],\n",
       "        [129.        ],\n",
       "        [125.5       ],\n",
       "        [186.33333333],\n",
       "        [191.        ],\n",
       "        [164.16666667],\n",
       "        [154.        ],\n",
       "        [155.66666667],\n",
       "        [165.5       ],\n",
       "        [164.33333333],\n",
       "        [156.16666667],\n",
       "        [141.66666667],\n",
       "        [117.5       ],\n",
       "        [129.83333333],\n",
       "        [156.66666667],\n",
       "        [133.83333333],\n",
       "        [115.16666667],\n",
       "        [111.16666667],\n",
       "        [115.5       ],\n",
       "        [116.5       ],\n",
       "        [115.        ],\n",
       "        [124.5       ],\n",
       "        [129.33333333],\n",
       "        [128.33333333],\n",
       "        [125.5       ],\n",
       "        [113.16666667],\n",
       "        [112.33333333],\n",
       "        [104.16666667],\n",
       "        [122.66666667],\n",
       "        [105.66666667],\n",
       "        [129.5       ],\n",
       "        [131.5       ],\n",
       "        [ 85.66666667],\n",
       "        [ 86.83333333],\n",
       "        [ 83.16666667],\n",
       "        [ 88.83333333],\n",
       "        [106.33333333],\n",
       "        [111.66666667],\n",
       "        [124.5       ],\n",
       "        [121.66666667],\n",
       "        [117.5       ],\n",
       "        [110.16666667],\n",
       "        [109.33333333],\n",
       "        [103.5       ],\n",
       "        [104.33333333],\n",
       "        [107.33333333],\n",
       "        [108.16666667],\n",
       "        [107.33333333],\n",
       "        [105.83333333],\n",
       "        [106.66666667],\n",
       "        [101.33333333],\n",
       "        [114.        ],\n",
       "        [118.33333333],\n",
       "        [149.5       ],\n",
       "        [148.33333333],\n",
       "        [162.33333333],\n",
       "        [176.33333333],\n",
       "        [177.5       ],\n",
       "        [167.16666667],\n",
       "        [152.5       ],\n",
       "        [131.66666667],\n",
       "        [101.66666667],\n",
       "        [111.83333333],\n",
       "        [136.33333333],\n",
       "        [114.83333333],\n",
       "        [106.        ],\n",
       "        [111.        ],\n",
       "        [112.16666667],\n",
       "        [113.        ],\n",
       "        [117.        ],\n",
       "        [120.16666667],\n",
       "        [119.66666667],\n",
       "        [116.66666667],\n",
       "        [108.5       ],\n",
       "        [122.        ],\n",
       "        [132.83333333],\n",
       "        [ 88.66666667],\n",
       "        [ 88.16666667],\n",
       "        [ 93.16666667],\n",
       "        [ 95.33333333],\n",
       "        [ 94.33333333],\n",
       "        [115.5       ],\n",
       "        [130.16666667],\n",
       "        [134.5       ],\n",
       "        [123.16666667],\n",
       "        [121.33333333],\n",
       "        [116.16666667],\n",
       "        [108.66666667],\n",
       "        [110.83333333],\n",
       "        [113.16666667],\n",
       "        [108.66666667],\n",
       "        [108.66666667],\n",
       "        [106.83333333],\n",
       "        [106.5       ],\n",
       "        [108.66666667],\n",
       "        [106.        ],\n",
       "        [102.        ],\n",
       "        [106.        ],\n",
       "        [163.83333333],\n",
       "        [188.83333333],\n",
       "        [187.16666667],\n",
       "        [176.5       ],\n",
       "        [136.5       ],\n",
       "        [142.16666667],\n",
       "        [169.        ],\n",
       "        [145.66666667],\n",
       "        [133.83333333],\n",
       "        [123.33333333],\n",
       "        [112.83333333],\n",
       "        [124.        ],\n",
       "        [107.5       ],\n",
       "        [119.16666667],\n",
       "        [141.83333333],\n",
       "        [131.83333333],\n",
       "        [110.66666667],\n",
       "        [ 96.16666667],\n",
       "        [ 96.33333333],\n",
       "        [105.5       ],\n",
       "        [110.        ],\n",
       "        [109.33333333],\n",
       "        [105.        ],\n",
       "        [ 89.16666667],\n",
       "        [104.16666667],\n",
       "        [108.        ],\n",
       "        [109.5       ],\n",
       "        [110.66666667],\n",
       "        [103.        ],\n",
       "        [105.33333333],\n",
       "        [105.        ],\n",
       "        [103.5       ],\n",
       "        [114.83333333],\n",
       "        [118.16666667],\n",
       "        [ 97.33333333],\n",
       "        [ 89.66666667],\n",
       "        [ 95.33333333],\n",
       "        [ 98.16666667],\n",
       "        [ 94.5       ],\n",
       "        [ 93.66666667],\n",
       "        [ 95.        ],\n",
       "        [ 97.66666667],\n",
       "        [105.5       ],\n",
       "        [109.5       ],\n",
       "        [109.5       ],\n",
       "        [108.        ],\n",
       "        [120.83333333],\n",
       "        [175.66666667],\n",
       "        [177.5       ],\n",
       "        [203.        ],\n",
       "        [207.16666667],\n",
       "        [190.66666667],\n",
       "        [170.33333333],\n",
       "        [153.33333333],\n",
       "        [139.5       ],\n",
       "        [167.66666667],\n",
       "        [169.        ],\n",
       "        [159.5       ],\n",
       "        [140.33333333],\n",
       "        [113.66666667],\n",
       "        [106.33333333],\n",
       "        [105.        ],\n",
       "        [110.5       ],\n",
       "        [117.33333333],\n",
       "        [118.83333333],\n",
       "        [121.33333333],\n",
       "        [123.5       ],\n",
       "        [124.33333333],\n",
       "        [126.5       ],\n",
       "        [124.83333333],\n",
       "        [123.5       ],\n",
       "        [115.5       ],\n",
       "        [105.33333333],\n",
       "        [ 97.66666667],\n",
       "        [101.        ],\n",
       "        [100.        ],\n",
       "        [ 92.66666667],\n",
       "        [ 90.        ],\n",
       "        [ 95.83333333],\n",
       "        [105.33333333],\n",
       "        [107.33333333],\n",
       "        [119.66666667],\n",
       "        [120.        ],\n",
       "        [112.        ],\n",
       "        [108.83333333],\n",
       "        [110.5       ],\n",
       "        [106.5       ],\n",
       "        [119.66666667],\n",
       "        [186.5       ],\n",
       "        [165.66666667],\n",
       "        [157.33333333],\n",
       "        [156.83333333],\n",
       "        [130.5       ],\n",
       "        [125.66666667],\n",
       "        [113.16666667],\n",
       "        [105.        ],\n",
       "        [102.83333333],\n",
       "        [103.5       ],\n",
       "        [143.83333333],\n",
       "        [148.16666667],\n",
       "        [139.        ],\n",
       "        [137.        ],\n",
       "        [149.83333333],\n",
       "        [145.66666667],\n",
       "        [146.66666667],\n",
       "        [134.5       ],\n",
       "        [129.83333333],\n",
       "        [114.33333333],\n",
       "        [110.33333333],\n",
       "        [114.        ],\n",
       "        [120.        ],\n",
       "        [121.        ],\n",
       "        [124.5       ],\n",
       "        [120.83333333],\n",
       "        [114.33333333],\n",
       "        [144.83333333],\n",
       "        [211.33333333],\n",
       "        [212.        ],\n",
       "        [175.        ],\n",
       "        [138.5       ],\n",
       "        [138.33333333],\n",
       "        [130.16666667],\n",
       "        [101.83333333],\n",
       "        [ 85.16666667],\n",
       "        [ 85.66666667],\n",
       "        [102.33333333],\n",
       "        [126.        ],\n",
       "        [132.        ],\n",
       "        [122.33333333],\n",
       "        [116.83333333],\n",
       "        [116.83333333],\n",
       "        [116.5       ],\n",
       "        [114.5       ],\n",
       "        [108.5       ],\n",
       "        [104.        ],\n",
       "        [103.16666667],\n",
       "        [101.        ],\n",
       "        [ 99.        ],\n",
       "        [ 97.83333333],\n",
       "        [ 96.33333333],\n",
       "        [ 96.        ],\n",
       "        [ 96.        ],\n",
       "        [ 96.        ],\n",
       "        [ 92.33333333],\n",
       "        [ 94.5       ],\n",
       "        [130.83333333],\n",
       "        [172.83333333],\n",
       "        [204.66666667],\n",
       "        [215.        ],\n",
       "        [196.33333333],\n",
       "        [180.16666667],\n",
       "        [178.        ],\n",
       "        [130.16666667],\n",
       "        [135.16666667],\n",
       "        [144.16666667],\n",
       "        [144.33333333],\n",
       "        [120.16666667],\n",
       "        [106.5       ],\n",
       "        [102.        ],\n",
       "        [107.66666667],\n",
       "        [116.        ],\n",
       "        [121.66666667],\n",
       "        [121.        ],\n",
       "        [121.66666667],\n",
       "        [120.33333333],\n",
       "        [117.5       ],\n",
       "        [114.16666667],\n",
       "        [115.33333333],\n",
       "        [109.        ],\n",
       "        [107.83333333],\n",
       "        [103.83333333],\n",
       "        [108.5       ],\n",
       "        [107.33333333],\n",
       "        [105.66666667],\n",
       "        [109.66666667],\n",
       "        [116.16666667],\n",
       "        [110.        ],\n",
       "        [ 99.16666667],\n",
       "        [ 97.5       ],\n",
       "        [ 97.        ],\n",
       "        [ 96.        ],\n",
       "        [ 96.16666667],\n",
       "        [ 92.16666667],\n",
       "        [ 88.33333333],\n",
       "        [ 95.66666667],\n",
       "        [ 97.83333333],\n",
       "        [104.33333333],\n",
       "        [102.16666667],\n",
       "        [ 98.16666667],\n",
       "        [100.83333333],\n",
       "        [ 95.        ],\n",
       "        [ 93.16666667],\n",
       "        [105.83333333],\n",
       "        [168.33333333],\n",
       "        [206.        ],\n",
       "        [191.5       ],\n",
       "        [187.16666667],\n",
       "        [184.66666667],\n",
       "        [187.16666667],\n",
       "        [177.33333333],\n",
       "        [151.        ],\n",
       "        [123.66666667],\n",
       "        [ 99.66666667],\n",
       "        [ 94.83333333],\n",
       "        [ 93.5       ],\n",
       "        [ 98.16666667],\n",
       "        [104.66666667],\n",
       "        [110.16666667],\n",
       "        [117.16666667],\n",
       "        [120.16666667],\n",
       "        [121.66666667],\n",
       "        [122.83333333],\n",
       "        [121.16666667],\n",
       "        [122.33333333],\n",
       "        [115.5       ],\n",
       "        [110.66666667],\n",
       "        [108.16666667],\n",
       "        [105.33333333],\n",
       "        [104.83333333],\n",
       "        [ 97.83333333],\n",
       "        [ 96.33333333],\n",
       "        [ 94.33333333],\n",
       "        [121.66666667],\n",
       "        [148.        ],\n",
       "        [142.        ],\n",
       "        [119.33333333],\n",
       "        [ 96.83333333],\n",
       "        [ 97.5       ],\n",
       "        [138.16666667],\n",
       "        [116.16666667],\n",
       "        [ 88.5       ],\n",
       "        [ 88.16666667],\n",
       "        [ 84.        ],\n",
       "        [ 94.5       ],\n",
       "        [ 97.66666667],\n",
       "        [105.5       ],\n",
       "        [183.        ],\n",
       "        [218.66666667],\n",
       "        [203.16666667],\n",
       "        [204.83333333],\n",
       "        [201.66666667],\n",
       "        [173.33333333],\n",
       "        [169.16666667],\n",
       "        [191.5       ],\n",
       "        [196.83333333],\n",
       "        [155.33333333],\n",
       "        [148.66666667],\n",
       "        [143.33333333],\n",
       "        [118.16666667],\n",
       "        [108.16666667],\n",
       "        [110.16666667],\n",
       "        [117.16666667],\n",
       "        [121.16666667],\n",
       "        [128.5       ],\n",
       "        [127.33333333],\n",
       "        [125.83333333],\n",
       "        [123.        ],\n",
       "        [114.5       ],\n",
       "        [117.83333333],\n",
       "        [130.        ],\n",
       "        [157.        ],\n",
       "        [151.66666667],\n",
       "        [128.66666667],\n",
       "        [ 97.33333333],\n",
       "        [ 89.        ],\n",
       "        [ 93.16666667],\n",
       "        [100.33333333],\n",
       "        [103.66666667],\n",
       "        [106.66666667],\n",
       "        [108.5       ],\n",
       "        [104.5       ],\n",
       "        [ 97.16666667],\n",
       "        [119.66666667],\n",
       "        [127.83333333],\n",
       "        [146.83333333],\n",
       "        [134.16666667],\n",
       "        [116.66666667],\n",
       "        [105.83333333],\n",
       "        [ 97.83333333],\n",
       "        [100.66666667],\n",
       "        [117.        ],\n",
       "        [130.33333333],\n",
       "        [138.83333333],\n",
       "        [171.33333333],\n",
       "        [164.83333333],\n",
       "        [164.83333333],\n",
       "        [173.16666667],\n",
       "        [177.33333333],\n",
       "        [160.33333333],\n",
       "        [136.        ],\n",
       "        [146.16666667],\n",
       "        [156.83333333],\n",
       "        [121.33333333],\n",
       "        [122.83333333],\n",
       "        [116.        ],\n",
       "        [118.5       ],\n",
       "        [118.        ],\n",
       "        [120.83333333],\n",
       "        [126.33333333],\n",
       "        [124.5       ],\n",
       "        [122.83333333],\n",
       "        [121.16666667],\n",
       "        [125.5       ],\n",
       "        [123.66666667],\n",
       "        [124.33333333],\n",
       "        [128.33333333],\n",
       "        [126.83333333],\n",
       "        [126.16666667],\n",
       "        [126.33333333],\n",
       "        [126.33333333],\n",
       "        [123.33333333],\n",
       "        [114.83333333],\n",
       "        [121.16666667],\n",
       "        [135.        ],\n",
       "        [128.33333333],\n",
       "        [133.83333333],\n",
       "        [122.16666667],\n",
       "        [102.5       ],\n",
       "        [112.33333333],\n",
       "        [115.83333333],\n",
       "        [115.33333333],\n",
       "        [117.        ],\n",
       "        [118.83333333],\n",
       "        [115.        ],\n",
       "        [111.5       ],\n",
       "        [109.16666667],\n",
       "        [110.33333333],\n",
       "        [105.5       ],\n",
       "        [110.5       ],\n",
       "        [108.83333333],\n",
       "        [114.33333333],\n",
       "        [202.33333333],\n",
       "        [207.        ],\n",
       "        [209.33333333],\n",
       "        [208.83333333],\n",
       "        [188.66666667],\n",
       "        [146.        ],\n",
       "        [159.83333333],\n",
       "        [183.5       ],\n",
       "        [170.83333333],\n",
       "        [168.83333333],\n",
       "        [126.        ],\n",
       "        [ 97.        ],\n",
       "        [ 92.5       ],\n",
       "        [ 93.        ],\n",
       "        [101.66666667],\n",
       "        [110.        ],\n",
       "        [113.33333333],\n",
       "        [101.83333333],\n",
       "        [103.        ],\n",
       "        [106.66666667],\n",
       "        [107.5       ],\n",
       "        [104.83333333],\n",
       "        [103.        ],\n",
       "        [106.33333333],\n",
       "        [149.83333333],\n",
       "        [138.33333333]]),\n",
       " array([[111.75892 ],\n",
       "        [111.46355 ],\n",
       "        [112.625824],\n",
       "        [119.10526 ],\n",
       "        [119.233955],\n",
       "        [116.46677 ],\n",
       "        [111.75892 ],\n",
       "        [108.715935],\n",
       "        [103.5467  ],\n",
       "        [113.01372 ],\n",
       "        [121.95016 ],\n",
       "        [116.11549 ],\n",
       "        [111.14878 ],\n",
       "        [113.01372 ],\n",
       "        [112.81319 ],\n",
       "        [114.35559 ],\n",
       "        [132.61221 ],\n",
       "        [129.45895 ],\n",
       "        [122.32161 ],\n",
       "        [128.94748 ],\n",
       "        [125.30149 ],\n",
       "        [126.861115],\n",
       "        [124.38372 ],\n",
       "        [175.57117 ],\n",
       "        [179.68576 ],\n",
       "        [156.29935 ],\n",
       "        [147.55948 ],\n",
       "        [148.98853 ],\n",
       "        [157.44954 ],\n",
       "        [156.44307 ],\n",
       "        [149.41751 ],\n",
       "        [137.1375  ],\n",
       "        [118.35114 ],\n",
       "        [127.47351 ],\n",
       "        [149.84665 ],\n",
       "        [130.49695 ],\n",
       "        [116.69808 ],\n",
       "        [114.030594],\n",
       "        [116.92347 ],\n",
       "        [117.62713 ],\n",
       "        [116.584656],\n",
       "        [123.705215],\n",
       "        [127.10603 ],\n",
       "        [126.37578 ],\n",
       "        [124.38372 ],\n",
       "        [115.334785],\n",
       "        [114.790665],\n",
       "        [109.90146 ],\n",
       "        [122.32161 ],\n",
       "        [111.14878 ],\n",
       "        [127.22851 ],\n",
       "        [128.6993  ],\n",
       "        [ 94.543884],\n",
       "        [ 94.98892 ],\n",
       "        [ 93.48054 ],\n",
       "        [ 95.74615 ],\n",
       "        [111.46355 ],\n",
       "        [114.35559 ],\n",
       "        [123.705215],\n",
       "        [121.56929 ],\n",
       "        [118.35114 ],\n",
       "        [113.43344 ],\n",
       "        [112.91103 ],\n",
       "        [109.00414 ],\n",
       "        [110.09209 ],\n",
       "        [111.89309 ],\n",
       "        [112.274925],\n",
       "        [111.89309 ],\n",
       "        [111.22462 ],\n",
       "        [111.62568 ],\n",
       "        [105.29231 ],\n",
       "        [115.88835 ],\n",
       "        [118.97658 ],\n",
       "        [143.72852 ],\n",
       "        [142.74722 ],\n",
       "        [154.71931 ],\n",
       "        [166.82852 ],\n",
       "        [167.84207 ],\n",
       "        [158.88858 ],\n",
       "        [146.26115 ],\n",
       "        [128.82224 ],\n",
       "        [105.86525 ],\n",
       "        [114.46435 ],\n",
       "        [132.61221 ],\n",
       "        [116.46677 ],\n",
       "        [111.30394 ],\n",
       "        [113.92762 ],\n",
       "        [114.68188 ],\n",
       "        [115.22594 ],\n",
       "        [117.989075],\n",
       "        [120.39949 ],\n",
       "        [120.00896 ],\n",
       "        [117.74776 ],\n",
       "        [112.44889 ],\n",
       "        [121.825836],\n",
       "        [129.71608 ],\n",
       "        [ 95.683   ],\n",
       "        [ 95.49362 ],\n",
       "        [ 97.73197 ],\n",
       "        [ 98.957596],\n",
       "        [ 98.33862 ],\n",
       "        [116.92347 ],\n",
       "        [127.71855 ],\n",
       "        [131.04926 ],\n",
       "        [122.69594 ],\n",
       "        [121.310715],\n",
       "        [117.3859  ],\n",
       "        [112.53592 ],\n",
       "        [113.829285],\n",
       "        [115.334785],\n",
       "        [112.53592 ],\n",
       "        [112.53592 ],\n",
       "        [111.69184 ],\n",
       "        [111.54408 ],\n",
       "        [112.53592 ],\n",
       "        [111.30394 ],\n",
       "        [106.450424],\n",
       "        [111.30394 ],\n",
       "        [156.01193 ],\n",
       "        [177.7741  ],\n",
       "        [176.30515 ],\n",
       "        [166.97327 ],\n",
       "        [132.75462 ],\n",
       "        [137.55957 ],\n",
       "        [160.47316 ],\n",
       "        [140.5073  ],\n",
       "        [130.49695 ],\n",
       "        [122.82345 ],\n",
       "        [115.117096],\n",
       "        [123.36613 ],\n",
       "        [111.962524],\n",
       "        [119.620125],\n",
       "        [137.27817 ],\n",
       "        [128.94748 ],\n",
       "        [113.73043 ],\n",
       "        [ 99.48531 ],\n",
       "        [ 99.60074 ],\n",
       "        [111.07296 ],\n",
       "        [113.335266],\n",
       "        [112.91104 ],\n",
       "        [110.785965],\n",
       "        [ 95.87246 ],\n",
       "        [109.90146 ],\n",
       "        [112.19148 ],\n",
       "        [113.01372 ],\n",
       "        [113.73043 ],\n",
       "        [108.1309  ],\n",
       "        [110.99715 ],\n",
       "        [110.785965],\n",
       "        [109.00414 ],\n",
       "        [116.46677 ],\n",
       "        [118.847916],\n",
       "        [100.40598 ],\n",
       "        [ 96.123634],\n",
       "        [ 98.957596],\n",
       "        [101.16937 ],\n",
       "        [ 98.44045 ],\n",
       "        [ 97.97615 ],\n",
       "        [ 98.74873 ],\n",
       "        [100.70731 ],\n",
       "        [111.07296 ],\n",
       "        [113.01372 ],\n",
       "        [113.01372 ],\n",
       "        [112.19148 ],\n",
       "        [120.92296 ],\n",
       "        [166.24963 ],\n",
       "        [167.84207 ],\n",
       "        [190.31393 ],\n",
       "        [194.02002 ],\n",
       "        [179.3915  ],\n",
       "        [161.62665 ],\n",
       "        [146.98378 ],\n",
       "        [135.30507 ],\n",
       "        [159.32056 ],\n",
       "        [160.47316 ],\n",
       "        [152.28088 ],\n",
       "        [136.01259 ],\n",
       "        [115.66501 ],\n",
       "        [111.46355 ],\n",
       "        [110.785965],\n",
       "        [113.63142 ],\n",
       "        [118.23044 ],\n",
       "        [119.36266 ],\n",
       "        [121.310715],\n",
       "        [122.95625 ],\n",
       "        [123.59217 ],\n",
       "        [125.07039 ],\n",
       "        [123.93133 ],\n",
       "        [122.95625 ],\n",
       "        [116.92347 ],\n",
       "        [110.99715 ],\n",
       "        [100.70731 ],\n",
       "        [104.74639 ],\n",
       "        [103.32348 ],\n",
       "        [ 97.507324],\n",
       "        [ 96.33332 ],\n",
       "        [ 99.27099 ],\n",
       "        [110.99715 ],\n",
       "        [111.89309 ],\n",
       "        [120.00896 ],\n",
       "        [120.2693  ],\n",
       "        [114.573105],\n",
       "        [112.625824],\n",
       "        [113.63142 ],\n",
       "        [111.54408 ],\n",
       "        [120.00896 ],\n",
       "        [175.71794 ],\n",
       "        [157.59338 ],\n",
       "        [150.41904 ],\n",
       "        [149.98973 ],\n",
       "        [127.96366 ],\n",
       "        [124.49685 ],\n",
       "        [115.334785],\n",
       "        [110.785965],\n",
       "        [107.843124],\n",
       "        [109.00414 ],\n",
       "        [138.9678  ],\n",
       "        [142.6071  ],\n",
       "        [134.87979 ],\n",
       "        [133.18196 ],\n",
       "        [144.009   ],\n",
       "        [140.5073  ],\n",
       "        [141.34663 ],\n",
       "        [131.04924 ],\n",
       "        [127.47351 ],\n",
       "        [116.11549 ],\n",
       "        [113.532425],\n",
       "        [115.88835 ],\n",
       "        [120.2693  ],\n",
       "        [121.0522  ],\n",
       "        [123.705215],\n",
       "        [120.92296 ],\n",
       "        [116.11549 ],\n",
       "        [139.81158 ],\n",
       "        [197.73404 ],\n",
       "        [198.32904 ],\n",
       "        [165.67097 ],\n",
       "        [134.45807 ],\n",
       "        [134.31754 ],\n",
       "        [127.71855 ],\n",
       "        [106.15364 ],\n",
       "        [ 94.35225 ],\n",
       "        [ 94.543884],\n",
       "        [107.02702 ],\n",
       "        [124.72441 ],\n",
       "        [129.07338 ],\n",
       "        [122.0743  ],\n",
       "        [117.8684  ],\n",
       "        [117.8684  ],\n",
       "        [117.62713 ],\n",
       "        [116.23103 ],\n",
       "        [112.44889 ],\n",
       "        [109.68219 ],\n",
       "        [108.422874],\n",
       "        [104.7464  ],\n",
       "        [102.074066],\n",
       "        [100.8607  ],\n",
       "        [ 99.60074 ],\n",
       "        [ 99.37574 ],\n",
       "        [ 99.37574 ],\n",
       "        [ 99.37574 ],\n",
       "        [ 97.36031 ],\n",
       "        [ 98.44045 ],\n",
       "        [128.20882 ],\n",
       "        [163.7919  ],\n",
       "        [191.79543 ],\n",
       "        [201.00888 ],\n",
       "        [184.401   ],\n",
       "        [170.16133 ],\n",
       "        [168.27667 ],\n",
       "        [127.71855 ],\n",
       "        [131.61595 ],\n",
       "        [139.25024 ],\n",
       "        [139.3915  ],\n",
       "        [120.39949 ],\n",
       "        [111.54408 ],\n",
       "        [106.450424],\n",
       "        [112.035995],\n",
       "        [117.26954 ],\n",
       "        [121.56929 ],\n",
       "        [121.0522  ],\n",
       "        [121.56929 ],\n",
       "        [120.527626],\n",
       "        [118.35114 ],\n",
       "        [116.00104 ],\n",
       "        [116.81077 ],\n",
       "        [112.71647 ],\n",
       "        [112.112   ],\n",
       "        [109.46646 ],\n",
       "        [112.44889 ],\n",
       "        [111.89309 ],\n",
       "        [111.14878 ],\n",
       "        [113.121956],\n",
       "        [117.3859  ],\n",
       "        [113.335266],\n",
       "        [102.27805 ],\n",
       "        [100.556206],\n",
       "        [100.10852 ],\n",
       "        [ 99.37574 ],\n",
       "        [ 99.48531 ],\n",
       "        [ 97.28703 ],\n",
       "        [ 95.55674 ],\n",
       "        [ 99.16651 ],\n",
       "        [100.8607  ],\n",
       "        [110.09209 ],\n",
       "        [106.75369 ],\n",
       "        [101.16937 ],\n",
       "        [104.48626 ],\n",
       "        [ 98.74873 ],\n",
       "        [ 97.73197 ],\n",
       "        [111.22462 ],\n",
       "        [159.89674 ],\n",
       "        [192.98152 ],\n",
       "        [180.12724 ],\n",
       "        [176.30515 ],\n",
       "        [174.10423 ],\n",
       "        [176.30515 ],\n",
       "        [167.69724 ],\n",
       "        [144.99123 ],\n",
       "        [123.0943  ],\n",
       "        [102.89905 ],\n",
       "        [ 98.644775],\n",
       "        [ 97.894745],\n",
       "        [101.16937 ],\n",
       "        [110.4761  ],\n",
       "        [113.43344 ],\n",
       "        [118.10974 ],\n",
       "        [120.39949 ],\n",
       "        [121.56929 ],\n",
       "        [122.445274],\n",
       "        [121.18145 ],\n",
       "        [122.0743  ],\n",
       "        [116.92347 ],\n",
       "        [113.73043 ],\n",
       "        [112.274925],\n",
       "        [110.99715 ],\n",
       "        [110.658066],\n",
       "        [100.8607  ],\n",
       "        [ 99.60074 ],\n",
       "        [ 98.33862 ],\n",
       "        [121.56929 ],\n",
       "        [142.46698 ],\n",
       "        [137.41888 ],\n",
       "        [119.74888 ],\n",
       "        [ 99.971344],\n",
       "        [100.556206],\n",
       "        [134.177   ],\n",
       "        [117.3859  ],\n",
       "        [ 95.619865],\n",
       "        [ 95.49362 ],\n",
       "        [ 93.85239 ],\n",
       "        [ 98.44045 ],\n",
       "        [100.70731 ],\n",
       "        [111.07296 ],\n",
       "        [172.63867 ],\n",
       "        [204.28972 ],\n",
       "        [190.46202 ],\n",
       "        [191.94363 ],\n",
       "        [189.12968 ],\n",
       "        [164.22531 ],\n",
       "        [160.61731 ],\n",
       "        [180.12724 ],\n",
       "        [184.84373 ],\n",
       "        [148.70259 ],\n",
       "        [143.02751 ],\n",
       "        [138.54498 ],\n",
       "        [118.847916],\n",
       "        [112.274925],\n",
       "        [113.43344 ],\n",
       "        [118.10974 ],\n",
       "        [121.18145 ],\n",
       "        [126.4952  ],\n",
       "        [125.65946 ],\n",
       "        [124.609985],\n",
       "        [122.56897 ],\n",
       "        [116.23103 ],\n",
       "        [118.59542 ],\n",
       "        [127.596016],\n",
       "        [150.13283 ],\n",
       "        [145.55281 ],\n",
       "        [126.61625 ],\n",
       "        [100.40598 ],\n",
       "        [ 95.8093  ],\n",
       "        [ 97.73197 ],\n",
       "        [103.77066 ],\n",
       "        [109.25089 ],\n",
       "        [111.62568 ],\n",
       "        [112.44889 ],\n",
       "        [110.28274 ],\n",
       "        [100.255775],\n",
       "        [120.00896 ],\n",
       "        [126.01756 ],\n",
       "        [141.48663 ],\n",
       "        [130.76898 ],\n",
       "        [117.74776 ],\n",
       "        [111.22462 ],\n",
       "        [100.8607  ],\n",
       "        [104.23679 ],\n",
       "        [117.989075],\n",
       "        [127.841095],\n",
       "        [134.73921 ],\n",
       "        [162.49239 ],\n",
       "        [156.87431 ],\n",
       "        [156.87431 ],\n",
       "        [164.08083 ],\n",
       "        [167.69724 ],\n",
       "        [152.99765 ],\n",
       "        [132.32742 ],\n",
       "        [140.92674 ],\n",
       "        [149.98973 ],\n",
       "        [121.310715],\n",
       "        [122.445274],\n",
       "        [117.26954 ],\n",
       "        [119.10526 ],\n",
       "        [118.71926 ],\n",
       "        [120.92296 ],\n",
       "        [124.955055],\n",
       "        [123.705215],\n",
       "        [122.445274],\n",
       "        [121.18145 ],\n",
       "        [124.38372 ],\n",
       "        [123.09431 ],\n",
       "        [123.59217 ],\n",
       "        [126.37578 ],\n",
       "        [125.30149 ],\n",
       "        [124.83974 ],\n",
       "        [124.955055],\n",
       "        [124.955055],\n",
       "        [122.82345 ],\n",
       "        [116.46677 ],\n",
       "        [121.18145 ],\n",
       "        [131.47424 ],\n",
       "        [126.37578 ],\n",
       "        [130.49695 ],\n",
       "        [121.95016 ],\n",
       "        [107.30574 ],\n",
       "        [114.790665],\n",
       "        [117.15417 ],\n",
       "        [116.81077 ],\n",
       "        [117.989075],\n",
       "        [119.36266 ],\n",
       "        [116.584656],\n",
       "        [114.24686 ],\n",
       "        [112.81319 ],\n",
       "        [113.532425],\n",
       "        [111.07296 ],\n",
       "        [113.63142 ],\n",
       "        [112.625824],\n",
       "        [116.11549 ],\n",
       "        [189.72171 ],\n",
       "        [193.87163 ],\n",
       "        [195.95033 ],\n",
       "        [195.50468 ],\n",
       "        [177.62714 ],\n",
       "        [140.78682 ],\n",
       "        [152.56755 ],\n",
       "        [173.0782  ],\n",
       "        [162.05945 ],\n",
       "        [160.32904 ],\n",
       "        [124.72441 ],\n",
       "        [100.10852 ],\n",
       "        [ 97.43367 ],\n",
       "        [ 97.65591 ],\n",
       "        [105.86525 ],\n",
       "        [113.335266],\n",
       "        [115.44457 ],\n",
       "        [106.15364 ],\n",
       "        [108.1309  ],\n",
       "        [111.62568 ],\n",
       "        [111.962524],\n",
       "        [110.658066],\n",
       "        [108.1309  ],\n",
       "        [111.46355 ],\n",
       "        [144.009   ]], dtype=float32))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "from keras.models import load_model\n",
    "model = load_model('prediabetic_lstm_cgm.h5')\n",
    "\n",
    "predict_by_model(model, data, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
